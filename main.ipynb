{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip \n",
    "%matplotlib ipympl\n",
    "import numpy as np\n",
    "import pandas as pd, pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "from librosa.effects import trim\n",
    "from librosa import power_to_db\n",
    "from librosa.feature import melspectrogram\n",
    "import IPython.display as ipd\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "import nlpaug.augmenter.audio as naa\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, ParameterGrid, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv(\"Materiale/development.csv\")\n",
    "eval = pd.read_csv(\"Materiale/evaluation.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSYS OF THE LABELED FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dev.isna().any(axis=0))\n",
    "print(dev.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataframe is complete, there is no need in having to do with missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['action'].value_counts().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['object'].value_counts().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in dev['action'].unique():\n",
    "    print(\"[\", action, \"]:\", dev.query(\"action == @action\")['object'].unique()) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create the label that will be used for the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dev['action'] + dev['object']\n",
    "dev['Predicted'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "dev['Predicted'].value_counts().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['Predicted'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's change the path using the one that we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"Materiale\"\n",
    "dev['path'] = dev['path'].str.replace(\"dsl_data\", dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dev['Predicted'], return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now add the file audio in wav format in the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioFile = []\n",
    "rateArr = []\n",
    "\n",
    "for path in dev['path']: \n",
    "    data, sr = librosa.load(path, sr=16000, dtype='float32')\n",
    "    AudioFile.append(data)\n",
    "    rateArr.append(sr)\n",
    "    \n",
    "# Some of the files are with a sample rate of 22050 (300), while all the others are 16000. \n",
    "# For this reason it was decided to use, instead of using scipy.io.wavfile.read, librosa.load, \n",
    "# which allows us to convert files with a rate different from the default rate (which we set = 16000 \n",
    "# being the most frequent one considering test sets and evaluation sets) into the rate defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if all the audio files are mono or stereo \n",
    "stereo = False\n",
    "for audio in AudioFile:\n",
    "    if audio.shape[0] == 2:\n",
    "        stereo = True\n",
    "        print(f\"Stereo = {stereo}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stereo == True:\n",
    "    i = 0\n",
    "    for audio in AudioFile:\n",
    "        if audio.shape[0] != 2:\n",
    "            AudioFile[i] = np.concatenate([audio, audio])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['Audio File'] = AudioFile\n",
    "#dev['Rate'] = rateArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['Current language used for work/school'].value_counts().sort_values().plot(kind=\"barh\")\n",
    "np.unique(dev['Current language used for work/school'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "dev['First Language spoken'].value_counts().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dev['First Language spoken'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = dev[dev['First Language spoken'] == 'English (United States)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['Current language used for work/school'].value_counts().sort_values().plot(kind=\"barh\")\n",
    "np.unique(dev['Current language used for work/school'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['Self-reported fluency level '].value_counts().sort_values().plot(kind=\"barh\")\n",
    "np.unique(dev['Self-reported fluency level '], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lenghts = list(map(lambda x: len(x), dev[\"Audio File\"]))\n",
    "max(x_lenghts), min(x_lenghts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La differenza massima che abbiamo tra i file Ã¨ di 19 secondi (300.000 samples con una frequenza di 16000Hz) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000\n",
    "def show_length_distribution(signals, rate=sr):\n",
    "    sample_times = [len(x)/sr for x in signals]\n",
    "\n",
    "    sns.set()\n",
    "    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.20, .80)})\n",
    "\n",
    "    # Add a graph in each part\n",
    "    sns.boxplot(x=sample_times, ax=ax_box, linewidth=0.9, color='#9af772')\n",
    "    sns.histplot(x=sample_times, ax=ax_hist, bins='fd', kde=True)\n",
    "\n",
    "    # Remove x axis name for the boxplot\n",
    "    ax_box.set(xlabel='')\n",
    "\n",
    "\n",
    "    title = 'Audio signal lengths'\n",
    "    x_label = 'duration (seconds)'\n",
    "    y_label = 'count'\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    ax_hist.set_xlabel(x_label)\n",
    "    ax_hist.set_ylabel(y_label)\n",
    "    plt.show()\n",
    "    return sample_times\n",
    "\n",
    "\n",
    "lengths = show_length_distribution(dev[\"Audio File\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 95\n",
    "np.percentile(lengths, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_outliers = sum(map(lambda x: x > np.percentile(lengths, q), lengths))\n",
    "tot_outliers\n",
    "#Questi sono gli outliers che abbiamo considerando il 90 percentile, valuteremo questi in modo differente a tempo debito. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_of_1s = stats.percentileofscore(x_lenghts, sr)\n",
    "print(percentile_of_1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Longest_audio = np.argmax([len(x) for x in dev[\"Audio File\"]])\n",
    "plt.plot(dev[\"Audio File\"][Longest_audio])\n",
    "dev[\"Audio File\"][Longest_audio]\n",
    "# plt.axhline(y=3, color='r', linestyle='-')\n",
    "\n",
    "ipd.Audio(dev[\"Audio File\"][Longest_audio], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shortest_audio = np.argmin([len(x) for x in dev[\"Audio File\"]])\n",
    "plt.plot(dev[\"Audio File\"][Shortest_audio])\n",
    "plt.title(\"Shortest audio signal\")\n",
    "\n",
    "ipd.Audio(dev[\"Audio File\"][Shortest_audio], rate=sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will remove the leading and trailing silence from signals to see if we get different distribution of length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default anything below 10 db is considered as silence\n",
    "def remove_silence(sample, sr=sr, top_db=10):\n",
    "    \"\"\"This function removes trailing and leading silence periods of audio signals.\n",
    "    \"\"\"\n",
    "    y = np.array(sample, dtype=np.float64)\n",
    "    # Trim the beginning and ending silence\n",
    "    yt, _ = trim(y, top_db=top_db)\n",
    "    return yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['Audio File'] = [remove_silence(x) for x in dev[\"Audio File\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lenghts = list(map(lambda x: len(x), dev[\"Audio File\"]))\n",
    "#print(x_lenghts)\n",
    "#np.unique(x_lenghts, return_counts=True)\n",
    "print(max(x_lenghts), min(x_lenghts))\n",
    "lengths = show_length_distribution(dev['Audio File'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(lengths, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_outliers = sum(map(lambda x: x > np.percentile(lengths, q), lengths))\n",
    "tot_outliers\n",
    "#Questi sono gli outliers che abbiamo considerando il 90 percentile, valuteremo questi in modo differente a tempo debito."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We considered audio files with duration shorter than 0.2 seconds to be useless, 0.2 is an arbitrary choice though, as we found out that lots of audios between 0.2 and 0.3 are records of 'play' or 'heat up', so we decided to keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['Duration'] = lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dev.sort_values('Duration', ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(df1['Audio File'][10], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Longest_audio = np.argmax([len(x) for x in dev['Audio File']])\n",
    "plt.plot(dev['Audio File'][Longest_audio])\n",
    "dev['Audio File'][Longest_audio]\n",
    "# plt.axhline(y=3, color='r', linestyle='-')\n",
    "\n",
    "ipd.Audio(dev['Audio File'][Longest_audio], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, _ in dev[(dev['Duration'] <= 0.2)].iterrows():\n",
    "    # print(id)\n",
    "    dev.drop([id], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, _ in dev[(dev['Duration'] >= np.percentile(lengths, q))].iterrows():\n",
    "    # print(id)\n",
    "    dev.drop([id], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Longest_audio = np.argmax([len(x) for x in dev['Audio File']])\n",
    "plt.plot(dev['Audio File'][Longest_audio])\n",
    "dev['Audio File'][Longest_audio]\n",
    "# plt.axhline(y=3, color='r', linestyle='-')\n",
    "\n",
    "ipd.Audio(dev['Audio File'][Longest_audio], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(v, l):\n",
    "    if l >= len(v):\n",
    "        return np.pad(v, (0, l-len(v)), constant_values=0.0)\n",
    "    return v[:l]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS OF THE FILES THAT HAVE TO BE PREDICTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval['path'] = eval['path'].str.replace(\"dsl_data\", dst)\n",
    "\n",
    "AudioFile2 = []\n",
    "rateArr2 = []\n",
    "\n",
    "for path in eval['path']: \n",
    "    data, sr = librosa.load(path, sr=16000, dtype='float32')\n",
    "    AudioFile2.append(data)\n",
    "    rateArr2.append(sr)\n",
    "    \n",
    "eval['Audio File'] = AudioFile2\n",
    "# eval['Rate'] = rateArr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(rateArr2))\n",
    "print(rateArr2.count(16000))\n",
    "print(rateArr2.count(22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in eval['speakerId'].unique():\n",
    "    print(dev.query(\"speakerId == @item\")['speakerId'].count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli speaker non sono gli stessi del file precedente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenghts2 = show_length_distribution(eval['Audio File'])\n",
    "eval['Duration'] = lenghts2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These audio files have different lenghts than those labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval['Audio File'] = [remove_silence(x) for x in eval[\"Audio File\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Longest_audio_eval = np.argmax([len(x) for x in eval['Audio File']])\n",
    "l = max(len(dev['Audio File'][Longest_audio]), len(eval['Audio File'][Longest_audio_eval]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "y = dev['Audio File'][Longest_audio]\n",
    "\n",
    "S = librosa.feature.melspectrogram(y=y, sr=sr, power=1)\n",
    "log_S = librosa.amplitude_to_db(S, ref=np.max)\n",
    "pcen_S = librosa.pcen(S * (2**31))\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n",
    "img = librosa.display.specshow(log_S, x_axis='time', y_axis='mel', ax=ax[0])\n",
    "ax[0].set(title='log amplitude (dB)', xlabel=None)\n",
    "ax[0].label_outer()\n",
    "imgpcen = librosa.display.specshow(pcen_S, x_axis='time', y_axis='mel', ax=ax[1])\n",
    "ax[1].set(title='Per-channel energy normalisation')\n",
    "fig.colorbar(img, ax=ax[0], format=\"%+2.0f dB\")\n",
    "fig.colorbar(imgpcen, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['Audio File'] = [pad_audio(x, l) for x in dev['Audio File']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval['Audio File'] = [pad_audio(x, l) for x in eval['Audio File']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(x) for x in dev['Audio File']]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(x) for x in eval['Audio File']]).unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 42\n",
    "y_dev = pd.DataFrame(dev['Predicted'], columns=['Predicted'])\n",
    "X_dev = dev[['Audio File', 'Predicted']]\n",
    "X_eval = eval[['Audio File']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(X: pd.DataFrame, y: pd.DataFrame) -> tuple:\n",
    "    augmented_X = pd.DataFrame(columns=X.columns)\n",
    "    augmented_y = pd.DataFrame(columns=y.columns)\n",
    "    highest_pred = X['Predicted'].value_counts().index[0]\n",
    "    highest_num = X['Predicted'].value_counts()[0]\n",
    "    \n",
    "    for pred in X.query('Predicted != @highest_pred')['Predicted'].unique():\n",
    "        x = X.query('Predicted == @pred')\n",
    "        len = x['Audio File'].shape[0]\n",
    "        n = highest_num - len\n",
    "        n1 = int(n/2)\n",
    "        n2 = n - n1\n",
    "        replace = n > len\n",
    "        speed_sample = x.sample(n=n1, random_state=rs, replace=replace)\n",
    "        speed_aug = naa.SpeedAug(zone=(0, 1), factor=(0.9, 1.1))\n",
    "        for row in speed_sample.itertuples(index=False):\n",
    "            aug_data = np.array(speed_aug.augment(row[0])).reshape(-1)\n",
    "            aug_data = pad_audio(aug_data, l)\n",
    "            new_row = pd.DataFrame({'Audio File': [aug_data], 'Predicted': [pred]})\n",
    "            augmented_X = pd.concat([augmented_X, new_row], ignore_index=True)\n",
    "            augmented_y = pd.concat([augmented_y, pd.DataFrame({'Predicted': [pred]})], ignore_index=True)\n",
    "        \n",
    "        pitch_sample = x.sample(n=n2, random_state=rs, replace=replace)\n",
    "        pitch_aug = naa.PitchAug(sampling_rate=sr, zone=(0, 1), factor=(0.9, 1.1))\n",
    "        for row in pitch_sample.itertuples(index=False):\n",
    "            aug_data = np.array(pitch_aug.augment(row[0])).reshape(-1)\n",
    "            new_row = pd.DataFrame({'Audio File': [aug_data], 'Predicted': [pred]})\n",
    "            augmented_X = pd.concat([augmented_X, new_row], ignore_index=True)\n",
    "            augmented_y = pd.concat([augmented_y, pd.DataFrame({'Predicted': [pred]})], ignore_index=True)\n",
    "    \n",
    "    return augmented_X, augmented_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_aug_tmp, y_aug_tmp = data_augmentation(X_dev.head(10), y_dev.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tmp = pd.concat([X_dev.head(10), X_aug_tmp])\n",
    "# y_tmp = pd.concat([y_dev.head(10), y_aug_tmp])\n",
    "# X_tmp['Predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_tmp['Predicted'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_mean_std(X, n, frame_size = 2048, hop_length = 512, n_mfcc = 13):\n",
    "    \"\"\"Computes mean and std of each n x n block of spectrograms of X\n",
    "       empty bins contains mean values of that column matrices\n",
    "       \n",
    "    Parameters:\n",
    "        X: 2-d sampling array\n",
    "        n: number of rows or columns to split spectogram\n",
    "    Returns:\n",
    "        A 2-d numpy array - feature Matrix with n x 2 x n features as columns\n",
    "    \"\"\"\n",
    "    X_sp = [] #feature matrix\n",
    "    for signal in X:\n",
    "        S = librosa.feature.melspectrogram(y=signal, sr=sr, n_fft=frame_size, hop_length=hop_length, n_mels=40, power=1)\n",
    "        log_S = librosa.amplitude_to_db(S=S, ref=np.max)\n",
    "        pcen_S = librosa.pcen(S=S*(2**31), sr=sr)\n",
    "        mfccs = librosa.feature.mfcc(y=signal, n_mfcc=n_mfcc, sr=sr)\n",
    "        delta_mfccs = librosa.feature.delta(mfccs)\n",
    "        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "        rms = librosa.feature.rms(y=signal).reshape(-1)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=signal).reshape(-1)\n",
    "        sp_cen = librosa.feature.spectral_centroid(y=signal, sr=sr, n_fft=frame_size, hop_length=hop_length).reshape(-1)\n",
    "        sp_bw = librosa.feature.spectral_bandwidth(y=signal, sr=sr, n_fft=frame_size, hop_length=hop_length).reshape(-1)\n",
    "        x_sp = [] #current feature set\n",
    "        \n",
    "        # split the rows\n",
    "        for v_split in np.array_split(pcen_S, n, axis=0):\n",
    "            # split the columns\n",
    "            for h_split in np.array_split(v_split, n, axis=1):\n",
    "                if h_split.size == 0: #happens when number of columns < n\n",
    "                    m = np.median(v_split).__round__(4)\n",
    "                    sd = np.std(v_split).__round__(4)\n",
    "                else:\n",
    "                    m = np.mean(h_split).__round__(4)\n",
    "                    sd = np.std(h_split).__round__(4)\n",
    "                x_sp.extend([m, sd])\n",
    "                \n",
    "        for v_split in np.array_split(mfccs, n_mfcc, axis=0):\n",
    "            # split the columns\n",
    "            for h_split in np.array_split(v_split, n, axis=1):\n",
    "                if h_split.size == 0: #happens when number of columns < n\n",
    "                    m = np.median(v_split).__round__(4)\n",
    "                    sd = np.std(v_split).__round__(4)\n",
    "                else:\n",
    "                    m = np.mean(h_split).__round__(4)\n",
    "                    sd = np.std(h_split).__round__(4)\n",
    "                x_sp.extend([m, sd])\n",
    "                \n",
    "        for v_split in np.array_split(delta_mfccs, n_mfcc, axis=0):\n",
    "            # split the columns\n",
    "            for h_split in np.array_split(v_split, n, axis=1):\n",
    "                if h_split.size == 0: #happens when number of columns < n\n",
    "                    m = np.median(v_split).__round__(4)\n",
    "                    sd = np.std(v_split).__round__(4)\n",
    "                else:\n",
    "                    m = np.mean(h_split).__round__(4)\n",
    "                    sd = np.std(h_split).__round__(4)\n",
    "                x_sp.extend([m, sd])\n",
    "                \n",
    "        for v_split in np.array_split(delta2_mfccs, n_mfcc, axis=0):\n",
    "            # split the columns\n",
    "            for h_split in np.array_split(v_split, n, axis=1):\n",
    "                if h_split.size == 0: #happens when number of columns < n\n",
    "                    m = np.median(v_split).__round__(4)\n",
    "                    sd = np.std(v_split).__round__(4)\n",
    "                else:\n",
    "                    m = np.mean(h_split).__round__(4)\n",
    "                    sd = np.std(h_split).__round__(4)\n",
    "                x_sp.extend([m, sd])\n",
    "                \n",
    "        for h_split in np.array_split(rms, n):\n",
    "            if h_split.size == 0: #happens when number of columns < n\n",
    "                m = np.median(h_split).__round__(4)\n",
    "                sd = np.std(h_split).__round__(4)\n",
    "            else:\n",
    "                m = np.mean(h_split).__round__(4)\n",
    "                sd = np.std(h_split).__round__(4)\n",
    "            x_sp.extend([m, sd])\n",
    "            \n",
    "        for h_split in np.array_split(zcr, n):\n",
    "            if h_split.size == 0: #happens when number of columns < n\n",
    "                m = np.median(h_split).__round__(4)\n",
    "                sd = np.std(h_split).__round__(4)\n",
    "            else:\n",
    "                m = np.mean(h_split).__round__(4)\n",
    "                sd = np.std(h_split).__round__(4)\n",
    "            x_sp.extend([m, sd])\n",
    "            \n",
    "        for h_split in np.array_split(sp_cen, n):\n",
    "            if h_split.size == 0: #happens when number of columns < n\n",
    "                m = np.median(h_split).__round__(4)\n",
    "                sd = np.std(h_split).__round__(4)\n",
    "            else:\n",
    "                m = np.mean(h_split).__round__(4)\n",
    "                sd = np.std(h_split).__round__(4)\n",
    "            x_sp.extend([m, sd])\n",
    "        \n",
    "        for h_split in np.array_split(sp_bw, n):\n",
    "            if h_split.size == 0: #happens when number of columns < n\n",
    "                m = np.median(h_split).__round__(4)\n",
    "                sd = np.std(h_split).__round__(4)\n",
    "            else:\n",
    "                m = np.mean(h_split).__round__(4)\n",
    "                sd = np.std(h_split).__round__(4)\n",
    "            x_sp.extend([m, sd])\n",
    "            \n",
    "        X_sp.append(x_sp)\n",
    "\n",
    "    return np.array(X_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal = X_dev['Audio File'].iloc[0]\n",
    "# n_mfcc = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, sr1 = librosa.load(librosa.ex('robin'))\n",
    "# S = librosa.feature.melspectrogram(y=signal, sr=sr, power=1, n_mels=40)\n",
    "# log_S = librosa.amplitude_to_db(S, ref=np.max)\n",
    "# pcen_S = librosa.pcen(S * (2**31), sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcen_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_mean_std(X_dev['Audio File'].tail(100), 18)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code used to save the information of the data divided in bins \n",
    "\n",
    "#for n in range(4,25,2):\n",
    "#    X_ft = ft_mean_std(dev['Audio File'], n)\n",
    "#    savetxt('savedData/' + str(n) + '.csv', X_ft, delimiter=',')\n",
    "    \n",
    "#data = loadtxt('data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per testare il funzionamento del salvataggio del file csv \n",
    "\n",
    "#n = 4\n",
    "#X_ft1 = loadtxt('savedData/' + str(n) + '.csv', delimiter=',')\n",
    "#X_ft = ft_mean_std(dev['Audio File'], n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection number of bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"rfc\": RandomForestClassifier(random_state=rs),\n",
    "    \"svm\": Pipeline([('scaler', StandardScaler()), ('SVM', svm.SVC())])\n",
    "}\n",
    "scores = {}\n",
    "for n in range(6,27,4):\n",
    "    X_ft = loadtxt('savedData/X_ft' + str(n) + '.csv', delimiter=',')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ft, y_dev, test_size=0.20, random_state=rs)\n",
    "    score = []\n",
    "    for model in models:\n",
    "        clf = models[model]\n",
    "        clf.fit(X_train, y_train['Predicted'].to_numpy())\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        score.append((model, f1))\n",
    "    scores[n] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf_scores = [x[0][1] for x in scores.values()]\n",
    "svm_scores = [x[1][1] for x in scores.values()]\n",
    "x = scores.keys()\n",
    "\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(x, rf_scores, label = 'RF')\n",
    "ax.plot(x, svm_scores, label= 'SVM')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "fig.suptitle(\"Model evaluation on different n. of bins\")\n",
    "ax.set_xlabel(\"n. of bins\")\n",
    "ax.set_ylabel('mean f1 score')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final valuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 22\n",
    "X_ft = ft_mean_std(X_dev['Audio File'], n)\n",
    "# X_ft = loadtxt('savedData/X_ft' + str(n) + '.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt('savedData/X_ft' + str(n) + '.csv', X_ft, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 26\n",
    "X_ft = ft_mean_std(X_dev['Audio File'], n)\n",
    "savetxt('savedData/X_ft' + str(n) + '.csv', X_ft, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "pca = PCA().fit(X_ft)\n",
    "ax.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "ax.set_xlabel('number of components')\n",
    "ax.set_ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 18\n",
    "pca_value = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.2, random_state=rs, stratify=y_dev)\n",
    "X_ft_train_val, X_ft_test, _, _ = train_test_split(X_ft, y_dev, test_size=0.2, random_state=rs, stratify=y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [150, 200, 250],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [False]\n",
    "}\n",
    "pg = list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "accuracies1_rf = {}\n",
    "for i, params in enumerate(pg):\n",
    "    print(i)\n",
    "    clf = RandomForestClassifier(n_estimators=params['n_estimators'], criterion=params['criterion'], \n",
    "                                max_features=params['max_features'], bootstrap = params['bootstrap'],\n",
    "                                n_jobs=-1, random_state=rs)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=rs)\n",
    "    for j, (train_index, val_index) in enumerate(sss.split(X_train, y_train)):\n",
    "        X = X_train.iloc[train_index, :]\n",
    "        y = y_train.iloc[train_index]\n",
    "        X_val = X_train.iloc[val_index, :]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "        \n",
    "        X_ft_train = X_ft_train_val[train_index, :]\n",
    "        X_ft_val = X_ft_train_val[val_index, :]\n",
    "        \n",
    "        X_aug, y_aug = data_augmentation(X, y)\n",
    "        X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "        \n",
    "        y = np.concatenate([y, y_aug['Predicted']])\n",
    "        \n",
    "        X_ft_train = np.concatenate([X_ft_train, X_aug_ft])\n",
    "        \n",
    "        X_ft_norm = maxabs_s.fit_transform(X_ft_train)\n",
    "        X_ft_val_norm = maxabs_s.transform(X_ft_val)\n",
    "        \n",
    "        X_proj = pca.fit_transform(X_ft_norm)\n",
    "        X_val_proj = pca.transform(X_ft_val_norm)\n",
    "        clf.fit(X_proj, y)\n",
    "        # clf.fit(X_proj, y['Predicted'].to_numpy())\n",
    "        y_pred = clf.predict(X_val_proj)\n",
    "        accuracies1_rf[i] = accuracy_score(y_val, y_pred)\n",
    "        # print(accuracies1_rf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies1_rf = dict(sorted(accuracies1_rf.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "for i, (k, v) in enumerate(accuracies1_rf.items()):\n",
    "    print(f\"{i}) index_rf: {k}, parameters: {pg[k]}:\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "accuracies5_rf = {}\n",
    "for i, params in enumerate(pg):\n",
    "    print(i)\n",
    "    clf = RandomForestClassifier(n_estimators=params['n_estimators'], criterion=params['criterion'], \n",
    "                                max_features=params['max_features'], bootstrap = params['bootstrap'],\n",
    "                                n_jobs=-1, random_state=rs)\n",
    "    skf = StratifiedKFold(n_splits=cv, random_state=rs, shuffle=True)\n",
    "    accuracy = []\n",
    "    for j, (train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        X = X_train.iloc[train_index, :]\n",
    "        y = y_train.iloc[train_index]\n",
    "        X_val = X_train.iloc[val_index, :]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "        \n",
    "        X_ft_train = X_ft_train_val[train_index, :]\n",
    "        X_ft_val = X_ft_train_val[val_index, :]\n",
    "        \n",
    "        X_aug, y_aug = data_augmentation(X, y)\n",
    "        X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "        \n",
    "        y = np.concatenate([y, y_aug['Predicted']])\n",
    "        \n",
    "        X_ft_train = np.concatenate([X_ft_train, X_aug_ft])\n",
    "        \n",
    "        X_ft_norm = maxabs_s.fit_transform(X_ft_train)\n",
    "        X_ft_val_norm = maxabs_s.transform(X_ft_val)\n",
    "        \n",
    "        X_proj = pca.fit_transform(X_ft_norm)\n",
    "        X_val_proj = pca.transform(X_ft_val_norm)\n",
    "        clf.fit(X_proj, y)\n",
    "        # clf.fit(X_proj, y['Predicted'].to_numpy())\n",
    "        y_pred = clf.predict(X_val_proj)\n",
    "        accuracy.append(accuracy_score(y_val, y_pred))\n",
    "    accuracies5_rf[i] = np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies5_rf = dict(sorted(accuracies5_rf.items(), key=lambda item: item[1], reverse=True))\n",
    "for i, (k, v) in enumerate(accuracies5_rf.items()):\n",
    "    print(f\"{i}) index_rf: {k} parameters: {pg[k]}:\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "scores5_rf = {}\n",
    "for i, params in enumerate(pg):\n",
    "    print(i)\n",
    "    clf = RandomForestClassifier(n_estimators=params['n_estimators'], criterion=params['criterion'], \n",
    "                                max_features=params['max_features'], bootstrap = params['bootstrap'],\n",
    "                                n_jobs=-1, random_state=rs)\n",
    "    skf = StratifiedKFold(n_splits=cv, random_state=rs, shuffle=True)\n",
    "    f1score = []\n",
    "    accuracy = []\n",
    "    for j, (train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        X = X_train.iloc[train_index, :]\n",
    "        y = y_train.iloc[train_index]\n",
    "        X_val = X_train.iloc[val_index, :]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "        \n",
    "        X_ft_train = X_ft_train_val[train_index, :]\n",
    "        X_ft_val = X_ft_train_val[val_index, :]\n",
    "        \n",
    "        X_aug, y_aug = data_augmentation(X, y)\n",
    "        X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "        \n",
    "        y = np.concatenate([y, y_aug['Predicted']])\n",
    "        \n",
    "        X_ft_train = np.concatenate([X_ft_train, X_aug_ft])\n",
    "        \n",
    "        X_ft_norm = maxabs_s.fit_transform(X_ft_train)\n",
    "        X_ft_val_norm = maxabs_s.transform(X_ft_val)\n",
    "        \n",
    "        X_proj = pca.fit_transform(X_ft_norm)\n",
    "        X_val_proj = pca.transform(X_ft_val_norm)\n",
    "        clf.fit(X_proj, y)\n",
    "        # clf.fit(X_proj, y['Predicted'].to_numpy())\n",
    "        y_pred = clf.predict(X_val_proj)\n",
    "        f1score.append(f1_score(y_val, y_pred, average='weighted'))\n",
    "        accuracy.append(accuracy_score(y_val, y_pred))\n",
    "    scores5_rf[i] = [np.mean(f1score), np.mean(accuracy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores5_rf = dict(sorted(scores5_rf.items(), key=lambda item: item[1][0], reverse=True))\n",
    "\n",
    "for i, (k, v) in enumerate(scores5_rf.items()):\n",
    "    print(f\"{i}) index_rf: {k}, parameters: {pg[k]}:\")\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will use the all train_val dataset as a training set for evaluating the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_rf = [pg[8], pg[13], pg[14], pg[12], pg[6], pg[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "scores1_rf = {}\n",
    "for i, params in enumerate(pg_rf):\n",
    "    print(i)\n",
    "    clf = RandomForestClassifier(n_estimators=params['n_estimators'], criterion=params['criterion'], \n",
    "                                max_features=params['max_features'], bootstrap = params['bootstrap'],\n",
    "                                n_jobs=-1, random_state=rs)\n",
    "        \n",
    "    X_aug, y_aug = data_augmentation(X_train, y_train)\n",
    "    X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "    \n",
    "    y = np.concatenate([y_train, y_aug['Predicted']])\n",
    "    \n",
    "    X_ft_train = np.concatenate([X_ft_train, X_aug_ft])\n",
    "    \n",
    "    X_ft_norm = maxabs_s.fit_transform(X_ft_train)\n",
    "    X_ft_test_norm = maxabs_s.transform(X_ft_test)\n",
    "    \n",
    "    X_proj = pca.fit_transform(X_ft_norm)\n",
    "    X_test_proj = pca.transform(X_ft_test_norm)\n",
    "    clf.fit(X_proj, y)\n",
    "    # clf.fit(X_proj, y_train['Predicted'].to_numpy())\n",
    "    y_pred = clf.predict(X_test_proj)\n",
    "    \n",
    "    f1score = f1_score(y_test, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    scores = [f1score, acc]\n",
    "    scores1_rf[i] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1_rf = dict(sorted(scores1_rf.items(), key=lambda item: item[1][0], reverse=True))\n",
    "for i, (k, v) in enumerate(scores1_rf.items()):\n",
    "    print(f\"{i}) index: {k}, parameters: {pg_rf[k]}:\")\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results from Eval (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_to_use = eval.loc[:, ['Audio File']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_ft = ft_mean_std(eval_to_use['Audio File'], n_bins)\n",
    "#savetxt('savedData/eval' + str(n_bins) + '.csv', eval_ft, delimiter=',')\n",
    "    \n",
    "#data = loadtxt('data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = {'bootstrap': False, 'criterion': 'gini', 'max_features': 'sqrt', 'n_estimators': 250}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ft = loadtxt('savedData/eval' + str(n_bins) + '.csv', delimiter=',')\n",
    "print(n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=best_param['n_estimators'], criterion=best_param['criterion'], \n",
    "                                max_features=best_param['max_features'], bootstrap = best_param['bootstrap'],\n",
    "                                n_jobs=-1, random_state=rs)\n",
    "    \n",
    "X_aug, y_aug = data_augmentation(X_dev, y_dev)\n",
    "X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "\n",
    "y = np.concatenate([y_dev['Predicted'], y_aug['Predicted']])\n",
    "\n",
    "X_ft = np.concatenate([X_ft, X_aug_ft])\n",
    "\n",
    "X_ft_norm = maxabs_s.fit_transform(X_ft)\n",
    "eval_ft_norm = maxabs_s.transform(eval_ft)\n",
    "\n",
    "X_proj = pca.fit_transform(X_ft_norm)\n",
    "eval_ft_proj = pca.transform(eval_ft_norm)\n",
    "clf.fit(X_proj, y)\n",
    "# clf.fit(X_proj, y_dev['Predicted'].to_numpy())\n",
    "\n",
    "y_pred = clf.predict(eval_ft_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('savedData/res_rf_aug_no_dum_corrected_1.csv', 'w') as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for i in range(len(y_pred)):\n",
    "      f.write(\"%i,%s\\n\" % (i, y_pred[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the element to consider for the training doing the PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 18\n",
    "X_ft = loadtxt('savedData/X_ft' + str(n) + '.csv', delimiter=',')\n",
    "# X_ft = ft_mean_std(X_dev['Audio File'], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "pca = PCA().fit(X_ft)\n",
    "ax.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "ax.set_xlabel('number of components')\n",
    "ax.set_ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 18\n",
    "pca_value = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.2, random_state=rs, stratify=y_dev)\n",
    "X_ft_train_val, X_ft_test, _, _ = train_test_split(X_ft, y_dev, test_size=0.2, random_state=rs, stratify=y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'gamma': ['scale', 1, 0.1], \n",
    "    'kernel': ['rbf'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "pg = list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "accuracies1_svm = {}\n",
    "for i, params in enumerate(pg):\n",
    "    print(i)\n",
    "    clf = svm.SVC(C=params['C'], kernel=params['kernel'],\n",
    "                  gamma=params['gamma'], class_weight=params['class_weight'])\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=rs)\n",
    "    for j, (train_index, val_index) in enumerate(sss.split(X_train, y_train)):\n",
    "        X = X_train.iloc[train_index, :]\n",
    "        y = y_train.iloc[train_index]\n",
    "        X_val = X_train.iloc[val_index, :]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "        \n",
    "        X_ft_train = X_ft_train_val[train_index, :]\n",
    "        X_ft_val = X_ft_train_val[val_index, :]\n",
    "        \n",
    "        X_aug, y_aug = data_augmentation(X, y)\n",
    "        X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "        y = pd.concat([y, y_aug])\n",
    "        X_ft_train = np.concatenate([X_ft_train, X_aug_ft])\n",
    "        \n",
    "        X_ft_norm = maxabs_s.fit_transform(X_ft_train)\n",
    "        X_ft_val_norm = maxabs_s.transform(X_ft_val)\n",
    "        \n",
    "        X_proj = pca.fit_transform(X_ft_norm)\n",
    "        X_val_proj = pca.transform(X_ft_val_norm)\n",
    "        clf.fit(X_proj, y)\n",
    "        # clf.fit(X_proj, y['Predicted'].to_numpy())\n",
    "        y_pred = clf.predict(X_val_proj)\n",
    "        accuracies1_svm[i] = accuracy_score(y_val, y_pred)\n",
    "        print(accuracies1_svm[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies1_svm = dict(sorted(accuracies1_svm.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "for i, (k, v) in enumerate(accuracies1_svm.items()):\n",
    "    print(f\"{i}) index_svm: {k}, parameters: {pg[k]}:\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "accuracies5_svm = {}\n",
    "for i, params in enumerate(pg):\n",
    "    print(i)\n",
    "    clf = svm.SVC(C=params['C'], kernel=params['kernel'],\n",
    "                  gamma=params['gamma'], class_weight=params['class_weight'])\n",
    "    skf = StratifiedKFold(n_splits=cv, random_state=rs, shuffle=True)\n",
    "    accuracy = []\n",
    "    for j, (train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        X = X_train.iloc[train_index, :]\n",
    "        y = y_train.iloc[train_index]\n",
    "        X_val = X_train.iloc[val_index, :]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "        \n",
    "        X_ft_train = X_ft_train_val[train_index, :]\n",
    "        X_ft_val = X_ft_train_val[val_index, :]\n",
    "        \n",
    "        X_aug, y_aug = data_augmentation(X, y)\n",
    "        X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "        y = np.concatenate([y, y_aug])\n",
    "        X_ft_train = np.concatenate([X_ft_train, X_aug_ft])\n",
    "        \n",
    "        X_ft_norm = maxabs_s.fit_transform(X_ft_train)\n",
    "        X_ft_val_norm = maxabs_s.transform(X_ft_val)\n",
    "        \n",
    "        X_proj = pca.fit_transform(X_ft_norm)\n",
    "        X_val_proj = pca.transform(X_ft_val_norm)\n",
    "        clf.fit(X_proj, y)\n",
    "        # clf.fit(X_proj, y['Predicted'].to_numpy())\n",
    "        y_pred = clf.predict(X_val_proj)\n",
    "        accuracy.append(accuracy_score(y_val, y_pred))\n",
    "    accuracies5_svm[i] = np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies5_svm = dict(sorted(accuracies5_svm.items(), key=lambda item: item[1], reverse=True))\n",
    "for i, (k, v) in enumerate(accuracies5_svm.items()):\n",
    "    print(f\"{i}) index_svm: {k} parameters: {pg[k]}:\")\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Valutiamo anche l'f1-score weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "scores1_svm = {}\n",
    "for i, params in enumerate(pg):\n",
    "    print(i)\n",
    "    clf = svm.SVC(C=params['C'], kernel=params['kernel'],\n",
    "                  gamma=params['gamma'], class_weight=params['class_weight'])\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=rs)\n",
    "    for j, (train_index, val_index) in enumerate(sss.split(X_train, y_train)):\n",
    "        X = X_train.iloc[train_index, :]\n",
    "        y = y_train.iloc[train_index]\n",
    "        X_val = X_train.iloc[val_index, :]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "        \n",
    "        X_ft_train = X_ft_train_val[train_index, :]\n",
    "        X_ft_val = X_ft_train_val[val_index, :]\n",
    "        \n",
    "        X_aug, y_aug = data_augmentation(X, y)\n",
    "        X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "        y = np.concatenate([y, y_aug])\n",
    "        X_ft_train = np.concatenate([X_ft_train, X_aug_ft])\n",
    "        \n",
    "        X_ft_norm = maxabs_s.fit_transform(X_ft_train)\n",
    "        X_ft_val_norm = maxabs_s.transform(X_ft_val)\n",
    "        \n",
    "        X_proj = pca.fit_transform(X_ft_norm)\n",
    "        X_val_proj = pca.transform(X_ft_val_norm)\n",
    "        \n",
    "        clf.fit(X_proj, y)\n",
    "        # clf.fit(X_proj, y['Predicted'].to_numpy())\n",
    "        y_pred = clf.predict(X_val_proj)\n",
    "        f1score = f1_score(y_val, y_pred, average='weighted')\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        scores = [f1score, acc]\n",
    "        scores1_svm[i] = scores\n",
    "        # print(fscore)\n",
    "        # print(accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore1_svm = dict(sorted(scores1_svm.items(), key=lambda item: item[1][0], reverse=True))\n",
    "\n",
    "for i, (k, v) in enumerate(fscore1_svm.items()):\n",
    "    print(f\"{i}) index_svm: {k}, parameters: {pg[k]}:\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "scores5_svm = {}\n",
    "for i, params in enumerate(pg):\n",
    "    print(i)\n",
    "    clf = svm.SVC(C=params['C'], kernel=params['kernel'],\n",
    "                  gamma=params['gamma'], class_weight=params['class_weight'])\n",
    "    skf = StratifiedKFold(n_splits=cv, random_state=rs, shuffle=True)\n",
    "    f1score = []\n",
    "    accuracy = []\n",
    "    for j, (train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        X = X_train.iloc[train_index, :]\n",
    "        y = y_train.iloc[train_index]\n",
    "        X_val = X_train.iloc[val_index, :]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "        \n",
    "        X_ft_train = X_ft_train_val[train_index, :]\n",
    "        X_ft_val = X_ft_train_val[val_index, :]\n",
    "        \n",
    "        X_aug, y_aug = data_augmentation(X, y)\n",
    "        X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "        y = np.concatenate([y, y_aug])\n",
    "        X_ft_train = np.concatenate([X_ft_train, X_aug_ft])\n",
    "        \n",
    "        X_ft_norm = maxabs_s.fit_transform(X_ft_train)\n",
    "        X_ft_val_norm = maxabs_s.transform(X_ft_val)\n",
    "        \n",
    "        X_proj = pca.fit_transform(X_ft_norm)\n",
    "        X_val_proj = pca.transform(X_ft_val_norm)\n",
    "        \n",
    "        clf.fit(X_proj, y)\n",
    "        # clf.fit(X_proj, y['Predicted'].to_numpy())\n",
    "        y_pred = clf.predict(X_val_proj)\n",
    "        f1score.append(f1_score(y_val, y_pred, average='weighted'))\n",
    "        accuracy.append(accuracy_score(y_val, y_pred))\n",
    "    scores5_svm[i] = [np.mean(f1score), np.mean(accuracy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores5_svm = dict(sorted(scores5_svm.items(), key=lambda item: item[1][0], reverse=True))\n",
    "\n",
    "for i, (k, v) in enumerate(scores5_svm.items()):\n",
    "    print(f\"{i}) index_svm: {k}, parameters: {pg[k]}:\")\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will use the all train_val dataset as a training set for evaluating the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pg = [pg[11], pg[8], pg[17], pg[23], pg[14], pg[20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "scores1_svm = {}\n",
    "for i, params in enumerate(svm_pg):\n",
    "    print(i)\n",
    "    # print(params)\n",
    "    clf = svm.SVC(C=params['C'], kernel=params['kernel'],\n",
    "                  gamma=params['gamma'], class_weight=params['class_weight'])\n",
    "        \n",
    "    X_aug, y_aug = data_augmentation(X_train, y_train)\n",
    "    X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "    y = np.concatenate([y_train, y_aug])\n",
    "    X_ft_train = np.concatenate([X_ft_train, X_aug_ft])\n",
    "    \n",
    "    X_ft_norm = maxabs_s.fit_transform(X_ft_train)\n",
    "    X_ft_test_norm = maxabs_s.transform(X_ft_test)\n",
    "    \n",
    "    X_proj = pca.fit_transform(X_ft_norm)\n",
    "    X_test_proj = pca.transform(X_ft_test_norm)\n",
    "    # clf.fit(X_proj, y)\n",
    "    clf.fit(X_proj, y_train['Predicted'].to_numpy())\n",
    "    y_pred = clf.predict(X_test_proj)\n",
    "    \n",
    "    f1score = f1_score(y_test, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    scores = [f1score, acc]\n",
    "    scores1_svm[i] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1_svm = dict(sorted(scores1_svm.items(), key=lambda item: item[1][0], reverse=True))\n",
    "for i, (k, v) in enumerate(scores1_svm.items()):\n",
    "    print(f\"{i}) index_svm: {k} parameters: {svm_pg[k]}:\")\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results from Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_to_use = eval.loc[:, ['Audio File']]\n",
    "# eval_to_use.head, eval_dum.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_ft = ft_mean_std(eval_to_use['Audio File'], n_bins)\n",
    "#savetxt('savedData/eval' + str(n_bins) + '.csv', eval_ft, delimiter=',')\n",
    "    \n",
    "#data = loadtxt('data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savetxt('savedData/eval' + str(18) + '.csv', eval_ft, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = {'C': 10, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ft = loadtxt('savedData/eval' + str(n_bins) + '.csv', delimiter=',')\n",
    "# eval_ft_dum = np.concatenate([eval_ft, eval_dum], axis=1)\n",
    "print(n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=pca_value)\n",
    "maxabs_s = MaxAbsScaler()\n",
    "\n",
    "clf = svm.SVC(C=best_param['C'], kernel=best_param['kernel'],\n",
    "              gamma=best_param['gamma'], class_weight=best_param['class_weight'])\n",
    "    \n",
    "X_aug, y_aug = data_augmentation(X_dev, y_dev)\n",
    "X_aug_ft = ft_mean_std(X_aug['Audio File'], n_bins)\n",
    "\n",
    "y = np.concatenate([y_dev['Predicted'], y_aug['Predicted']])\n",
    "\n",
    "X_ft = np.concatenate([X_ft, X_aug_ft])\n",
    "\n",
    "X_ft_norm = maxabs_s.fit_transform(X_ft)\n",
    "eval_ft_norm = maxabs_s.transform(eval_ft)\n",
    "\n",
    "X_proj = pca.fit_transform(X_ft_norm)\n",
    "eval_ft_proj = pca.transform(eval_ft_norm)\n",
    "clf.fit(X_proj, y)\n",
    "# clf.fit(X_proj, y_dev['Predicted'].to_numpy())\n",
    "\n",
    "y_pred = clf.predict(eval_ft_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('savedData/res_svm_aug_no_dummies_1.csv', 'w') as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for i in range(len(y_pred)):\n",
    "      f.write(\"%i,%s\\n\" % (i, y_pred[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
